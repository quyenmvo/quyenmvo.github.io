[{"content":"Introduction Diffusion models have emerged as one of the most advanced methods in the field of generative modeling. They have broken the long-standing dominance of GANs in image generation tasks and have shown potential in various other fields, including computer vision, natural language processing, temporal data modeling, multimodal modeling, machine learning, and interdisciplinary applications such as computational chemistry and medical image reconstruction.\nDiffusion models have increasingly achieved significant successes across multiple domains. Recently, the Sora model demonstrated the capability to generate dynamic videos from text input alone. In this report, we will explore and elucidate the fundamentals of diffusion models, their image generation capabilities, and their architectural designs.\nIn this blog, we will explore the foundational concepts of diffusion models, the process of image generation, and the architecture of these models.\nDenoising Diffusion Probabilistic Models Forward diffusion process Given a data point sampled from a real data distribution \\(\\mathbf{x}_0 \\sim q(\\mathbf{x})\\), we define the forward diffusion process as the process that gradually destroys the data distribution by adding noise to the sample over \\(T\\) steps, creating a noisy sample sequence: \\(\\mathbf{x}_1, \\dots, \\mathbf{x}_T\\).\n$$\rq(\\mathbf{x}_t \\vert \\mathbf{x}_{t-1}) = \\mathcal{N}(\\mathbf{x}_t; \\sqrt{1 - \\beta_t} \\mathbf{x}_{t-1}, \\beta_t\\mathbf{I}) \\quad\rq(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_0) = \\prod^T_{t=1} q(\\mathbf{x}_t \\vert \\mathbf{x}_{t-1})\r$$ Note that \\(\\beta\\) here is a time-dependent hyperparameter controlled by the variance schedule \\(\\{\\beta_t \\in (0, 1)\\}_{t=1}^T\\). The data sample \\(\\mathbf{x}_0\\) gradually loses its recognizable features as \\(t\\) increases, and when \\(T \\to \\infty\\), \\(\\mathbf{x}_T\\) becomes equivalent to an isotropic Gaussian distribution.\nLet \\(\\alpha_t = 1 - \\beta_t\\) and \\(\\bar{\\alpha}_{t} = \\prod_{i=1}^t \\alpha_i\\). Rewriting (1), we get:\n$$\r\\begin{aligned}\r\\mathbf{x}_t \u0026= \\sqrt{1 - \\beta_t} \\mathbf{x}_{t-1} + \\beta_t \\boldsymbol{\\epsilon}_{t-1} \u0026 \\text{ ;with } \\boldsymbol{\\epsilon}_{t-1}, \\boldsymbol{\\epsilon}_{t-2}, \\dots \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}) \\\\\r\u0026= \\sqrt{\\alpha_t}\\mathbf{x}_{t-1} + \\sqrt{1 - \\alpha_t}\\boldsymbol{\\epsilon}_{t-1}\\\\\r\u0026= \\sqrt{\\alpha_t \\alpha_{t-1}} \\mathbf{x}_{t-2} + \\sqrt{1 - \\alpha_t}\\boldsymbol{\\epsilon}_{t-1} + \\sqrt{\\alpha_t (1 - \\alpha_{t-1})} {\\boldsymbol{\\epsilon}}_{t-2}\\\\\r\u0026= \\sqrt{\\alpha_t \\alpha_{t-1}} \\mathbf{x}_{t-2} + \\sqrt{1 - \\alpha_t \\alpha_{t-1}} \\bar{\\boldsymbol{\\epsilon}}_{t-2}\\\\\r\u0026= \\dots \\\\\r\u0026= \\sqrt{\\bar{\\alpha}_t}\\mathbf{x}_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\boldsymbol{\\epsilon}\\\\\rq(\\mathbf{x}_t \\vert \\mathbf{x}_0) \u0026= \\mathcal{N}(\\mathbf{x}_t; \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0, (1 - \\bar{\\alpha}_t)\\mathbf{I})\r\\end{aligned}\r$$ Generally, we need larger update steps as the sample becomes noisier, so \\(\\beta_1 \u003c \\beta_2 \u003c \\dots \u003c \\beta_T\\) and \\(\\bar{\\alpha}_1 \u003e \\dots \u003e \\bar{\\alpha}_T\\).\nReverse diffusion process We have the forward diffusion process that turns a sample \\(\\mathbf{x}_0\\) into Gaussian noise. If we can reverse this process using \\(q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t)\\), we can reconstruct the sample from Gaussian noise \\(\\mathbf{x}_T \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})\\). However, we cannot estimate \\(q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t)\\), so we will learn a model \\(p_\\theta(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t)\\) to approximate \\(q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t)\\).\n$$\rp_\\theta(\\mathbf{x}_{0:T}) = p(\\mathbf{x}_T) \\prod^T_{t=1} p_\\theta(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t) \\quad\rp_\\theta(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t) = \\mathcal{N}(\\mathbf{x}_{t-1}; \\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t), \\boldsymbol{\\Sigma}_\\theta(\\mathbf{x}_t, t))\r$$ Note that we can compute the reverse distribution when \\(\\mathbf{x}_0\\) is known as follows:\n$$\rq(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t, \\mathbf{x}_0) = \\mathcal{N}(\\mathbf{x}_{t-1}; \\tilde{\\boldsymbol{\\mu}}(\\mathbf{x}_t, \\mathbf{x}_0), \\tilde{\\beta}_t \\mathbf{I})\r$$ Applying Bayes\u0026rsquo; theorem, we get:\n$$\r\\begin{aligned}\rq(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t, \\mathbf{x}_0) \u0026= q(\\mathbf{x}_t \\vert \\mathbf{x}_{t-1}, \\mathbf{x}_0) \\frac{ q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_0) }{ q(\\mathbf{x}_t \\vert \\mathbf{x}_0) } \\\\\r\u0026\\propto \\exp \\Big(-\\frac{1}{2} \\big(\\frac{(\\mathbf{x}_t - \\sqrt{\\alpha_t} \\mathbf{x}_{t-1})^2}{\\beta_t} + \\frac{(\\mathbf{x}_{t-1} - \\sqrt{\\bar{\\alpha}_{t-1}} \\mathbf{x}_0)^2}{1-\\bar{\\alpha}_{t-1}} - \\frac{(\\mathbf{x}_t - \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0)^2}{1-\\bar{\\alpha}_t} \\big) \\Big) \\\\\r\u0026= \\exp \\Big(-\\frac{1}{2} \\big(\\frac{\\mathbf{x}_t^2 - 2\\sqrt{\\alpha_t} \\mathbf{x}_t \\mathbf{x}_{t-1} + \\alpha_t \\mathbf{x}_{t-1}^2 }{\\beta_t} + \\frac{ \\mathbf{x}_{t-1}^2 - 2 \\sqrt{\\bar{\\alpha}_{t-1}} \\mathbf{x}_0 \\mathbf{x}_{t-1} + \\bar{\\alpha}_{t-1} \\mathbf{x}_0^2 }{1-\\bar{\\alpha}_{t-1}} - \\frac{(\\mathbf{x}_{t} - \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0)^2}{1-\\bar{\\alpha}_t} \\big) \\Big) \\\\\r\u0026= \\exp\\Big( -\\frac{1}{2} \\big( (\\frac{\\alpha_t}{\\beta_t} + \\frac{1}{1 - \\bar{\\alpha}_{t-1}}) \\mathbf{x}_{t-1}^2 - (\\frac{2\\sqrt{\\alpha_t}}{\\beta_t} \\mathbf{x}_t + \\frac{2\\sqrt{\\bar{\\alpha}_{t-1}}}{1 - \\bar{\\alpha}_{t-1}} \\mathbf{x}_0) \\mathbf{x}_{t-1} + C(\\mathbf{x}_t, \\mathbf{x}_0) \\big) \\Big)\r\\end{aligned}\r$$ where \\(C(\\mathbf{x}_t, \\mathbf{x}_0)\\) is a function not involving \\(\\mathbf{x}_{t-1}\\). According to the probability density function of the Gaussian distribution, the mean and variance can be computed as follows:\n$$\r\\begin{aligned}\r\\tilde{\\boldsymbol{\\mu}}_t (\\mathbf{x}_t, \\mathbf{x}_0)\r\u0026= (\\frac{\\sqrt{\\alpha_t}}{\\beta_t} \\mathbf{x}_t + \\frac{\\sqrt{\\bar{\\alpha}_{t-1} }}{1 - \\bar{\\alpha}_{t-1}} \\mathbf{x}_0)/(\\frac{\\alpha_t}{\\beta_t} + \\frac{1}{1 - \\bar{\\alpha}_{t-1}}) \\\\\r\u0026= (\\frac{\\sqrt{\\alpha_t}}{\\beta_t} \\mathbf{x}_t + \\frac{\\sqrt{\\bar{\\alpha}_{t-1} }}{1 - \\bar{\\alpha}_{t-1}} \\mathbf{x}_0) \\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\cdot \\beta_t \\\\\r\u0026= \\frac{\\sqrt{\\alpha_t}(1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t} \\mathbf{x}_t + \\frac{\\sqrt{\\bar{\\alpha}_{t-1}}\\beta_t}{1 - \\bar{\\alpha}_t} \\mathbf{x}_0\\\\\r\\tilde{\\beta}_t \u0026= 1/(\\frac{\\alpha_t}{\\beta_t} + \\frac{1}{1 - \\bar{\\alpha}_{t-1}})\\\\\r\u0026= 1/(\\frac{\\alpha_t - \\alpha_t \\bar{\\alpha}_{t-1} + \\beta_t}{\\beta_t (1 - \\bar{\\alpha}_{t-1})}) \\\\\r\u0026= \\frac{\\beta_t (1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t} \\\\\rq(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t, \\mathbf{x}_0) \u0026= \\mathcal{N}(\\mathbf{x}_{t-1}; \\tilde{\\boldsymbol{\\mu}}_t (\\mathbf{x}_t, \\mathbf{x}_0), \\tilde{\\beta}_t \\mathbf{I})\r\\end{aligned}\r$$ Thus, we can parameterize the mean \\(\\tilde{\\boldsymbol{\\mu}}(\\mathbf{x}_t, \\mathbf{x}_0)\\) with a neural network with parameter \\(\\theta\\), and the variance with a fixed schedule \\(\\beta_t\\). A common approach is to parameterize \\(\\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t) = \\frac{1}{\\sqrt{\\alpha_t}} \\Big( \\mathbf{x}_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_\\theta(\\mathbf{x}_t, t) \\Big)\\).\nTraining Loss As mentioned above, we will learn a model \\(p_\\theta\\) to approximate \\(q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t)\\). We will minimize the negative log-likelihood: \\(-\\log p_\\theta(\\mathbf{x}_0)\\). However, this function is not directly computable, so we need to use a technique similar to the Variational Autoencoder (VAE) called the Variational Lower Bound:\n$$\r\\begin{aligned}\r\\log p_\\theta(\\mathbf{x}_0) \u0026\\leq - \\log p_\\theta(\\mathbf{x}_0) + D_\\text{KL}(q(\\mathbf{x}_{1:T}\\vert\\mathbf{x}_0) \\| p_\\theta(\\mathbf{x}_{1:T}\\vert\\mathbf{x}_0) ) \\\\\r\u0026= -\\log p_\\theta(\\mathbf{x}_0) + \\mathbb{E}_{q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_0)} \\Big[ \\log\\frac{q(\\mathbf{x}_{1:T}\\vert\\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_{0:T}) / p_\\theta(\\mathbf{x}_0)} \\Big] \\\\\r\u0026= -\\log p_\\theta(\\mathbf{x}_0) + \\mathbb{E}_{q(\\mathbf{x}_{1:T}\\vert\\mathbf{x}_0)} \\Big[ \\log\\frac{q(\\mathbf{x}_{1:T}\\vert\\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_{0:T})} + \\log p_\\theta(\\mathbf{x}_0) \\Big] \\\\\r\u0026= \\mathbb{E}_{q(\\mathbf{x}_{1:T}\\vert\\mathbf{x}_0)} \\Big[ \\log \\frac{q(\\mathbf{x}_{1:T}\\vert\\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_{0:T})} \\Big] \\\\\r\\text{Let } L_\\text{VLB} \u0026= \\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\Big[ \\log \\frac{q(\\mathbf{x}_{1:T}\\vert\\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_{0:T})} \\Big] \\geq - \\mathbb{E}_{q(\\mathbf{x}_0)} \\log p_\\theta(\\mathbf{x}_0)\r\\end{aligned}\r$$ The objective function can be rewritten as a sum of KL-divergences:\n$$\r\\begin{aligned}\rL_\\text{VLB} \u0026= \\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\Big[ \\log\\frac{q(\\mathbf{x}_{1:T}\\vert\\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_{0:T})} \\Big] \\\\\r\u0026= \\mathbb{E}_q \\Big[ \\log\\frac{\\prod_{t=1}^T q(\\mathbf{x}_t\\vert\\mathbf{x}_{t-1})}{ p_\\theta(\\mathbf{x}_T) \\prod_{t=1}^T p_\\theta(\\mathbf{x}_{t-1} \\vert\\mathbf{x}_t) } \\Big] \\\\\r\u0026= \\mathbb{E}_q \\Big[ -\\log p_\\theta(\\mathbf{x}_T) + \\sum_{t=1}^T \\log \\frac{q(\\mathbf{x}_t\\vert\\mathbf{x}_{t-1})}{p_\\theta(\\mathbf{x}_{t-1} \\vert\\mathbf{x}_t)} \\Big] \\\\\r\u0026= \\mathbb{E}_q \\Big[ -\\log p_\\theta(\\mathbf{x}_T) + \\sum_{t=2}^T \\log \\frac{q(\\mathbf{x}_t\\vert\\mathbf{x}_{t-1})}{p_\\theta(\\mathbf{x}_{t-1} \\vert\\mathbf{x}_t)} + \\log\\frac{q(\\mathbf{x}_1 \\vert \\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_0 \\vert \\mathbf{x}_1)} \\Big] \\\\\r\u0026= \\mathbb{E}_q \\Big[ -\\log p_\\theta(\\mathbf{x}_T) + \\sum_{t=2}^T \\log \\Big( \\frac{q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t, \\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_{t-1} \\vert\\mathbf{x}_t)}\\cdot \\frac{q(\\mathbf{x}_t \\vert \\mathbf{x}_0)}{q(\\mathbf{x}_{t-1}\\vert\\mathbf{x}_0)} \\Big) + \\log \\frac{q(\\mathbf{x}_1 \\vert \\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_0 \\vert \\mathbf{x}_1)} \\Big] \\\\\r\u0026= \\mathbb{E}_q \\Big[ -\\log p_\\theta(\\mathbf{x}_T) + \\sum_{t=2}^T \\log \\frac{q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t, \\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_{t-1} \\vert\\mathbf{x}_t)} + \\sum_{t=2}^T \\log \\frac{q(\\mathbf{x}_t \\vert \\mathbf{x}_0)}{q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_0)} + \\log\\frac{q(\\mathbf{x}_1 \\vert \\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_0 \\vert \\mathbf{x}_1)} \\Big] \\\\\r\u0026= \\mathbb{E}_q \\Big[ -\\log p_\\theta(\\mathbf{x}_T) + \\sum_{t=2}^T \\log \\frac{q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t, \\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_{t-1} \\vert\\mathbf{x}_t)} + \\log\\frac{q(\\mathbf{x}_T \\vert \\mathbf{x}_0)}{q(\\mathbf{x}_1 \\vert \\mathbf{x}_0)} + \\log \\frac{q(\\mathbf{x}_1 \\vert \\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_0 \\vert \\mathbf{x}_1)} \\Big] \\\\\r\u0026= \\mathbb{E}_q \\Big[ \\log\\frac{q(\\mathbf{x}_T \\vert \\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_T)} + \\sum_{t=2}^T \\log \\frac{q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t, \\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_{t-1} \\vert\\mathbf{x}_t)} - \\log p_\\theta(\\mathbf{x}_0 \\vert \\mathbf{x}_1) \\Big] \\\\\r\u0026= \\mathbb{E}_q [\\underbrace{D_\\text{KL}(q(\\mathbf{x}_T \\vert \\mathbf{x}_0) \\parallel p_\\theta(\\mathbf{x}_T))}_{L_T} + \\sum_{t=2}^T \\underbrace{D_\\text{KL}(q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t, \\mathbf{x}_0) \\parallel p_\\theta(\\mathbf{x}_{t-1} \\vert\\mathbf{x}_t))}_{L_{t-1}} \\underbrace{- \\log p_\\theta(\\mathbf{x}_0 \\vert \\mathbf{x}_1)}_{L_0} ]\\\\\r\u0026= \\mathbb{E}_q [L_T + L_{T-1} + \\dots + L_0 ] \\\\\\\\\r\\text{where: } L_T \u0026= D_\\text{KL}(q(\\mathbf{x}_T \\vert \\mathbf{x}_0) \\parallel p_\\theta(\\mathbf{x}_T)) \\\\\rL_t \u0026= D_\\text{KL}(q(\\mathbf{x}_t \\vert \\mathbf{x}_{t+1}, \\mathbf{x}_0) \\parallel p_\\theta(\\mathbf{x}_t \\vert\\mathbf{x}_{t+1})) \\text{ for }1 \\leq t \\leq T-1 \\\\\rL_0 \u0026= - \\log p_\\theta(\\mathbf{x}_0 \\vert \\mathbf{x}_1)\r\\end{aligned}\r$$ Each KL term in \\(L_\\text{VLB}\\) (except \\(L_0\\)) compares two Gaussian distributions. \\(L_T\\) is a constant so it can be ignored during training because \\(q\\) has no parameters to learn and \\(x_T\\) is just Gaussian noise.\nNeed to recall that we need to train a model capable of approximating the conditional distribution of the reverse diffusion process \\( p_\\theta(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t) = \\mathcal{N}(\\mathbf{x}_{t-1}; \\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t), \\boldsymbol{\\Sigma}_\\theta(\\mathbf{x}_t, t)) \\). We will train \\( \\boldsymbol{\\mu}_\\theta \\) to predict \\( \\tilde{\\boldsymbol{\\mu}}_t = \\frac{1}{\\sqrt{\\alpha_t}} \\Big( \\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\boldsymbol{\\epsilon}_t \\Big) \\). Since \\( \\mathbf{x}_t \\) is always known at timestep \\( t \\), we only need to predict \\( \\boldsymbol{\\epsilon}_t \\) at timestep \\( t \\) with inputs \\( \\mathbf{x}_t \\) and \\( t \\):\n$$\r\\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t) = \\frac{1}{\\sqrt{\\alpha_t}} \\Big( \\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t) \\Big)\r$$ This leads to:\n$$\r\\mathbf{x}_{t-1} = \\mathcal{N}(\\mathbf{x}_{t-1}; \\frac{1}{\\sqrt{\\alpha_t}} \\Big( \\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t) \\Big), \\boldsymbol{\\Sigma}_\\theta(\\mathbf{x}_t, t))\r$$ We aim to minimize the mean square error of \\( \\tilde{\\boldsymbol{\\mu}} \\):\n$$\r\\begin{aligned}\rL_t \u0026= \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\Big[\\frac{1}{2 \\| \\boldsymbol{\\Sigma}_\\theta(\\mathbf{x}_t, t) \\|^2_2} \\| \\tilde{\\boldsymbol{\\mu}}_t(\\mathbf{x}_t, \\mathbf{x}_0) - \\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t) \\|^2 \\Big] \\\\\r\u0026= \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\Big[\\frac{ (1 - \\alpha_t)^2 }{2 \\alpha_t (1 - \\bar{\\alpha}_t) \\| \\boldsymbol{\\Sigma}_\\theta \\|^2_2} \\|\\boldsymbol{\\epsilon}_t - \\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t)\\|^2 \\Big] \\\\\r\u0026= \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\Big[\\frac{ (1 - \\alpha_t)^2 }{2 \\alpha_t (1 - \\bar{\\alpha}_t) \\| \\boldsymbol{\\Sigma}_\\theta \\|^2_2} \\|\\boldsymbol{\\epsilon}_t - \\boldsymbol{\\epsilon}_\\theta(\\sqrt{\\bar{\\alpha}_t}\\mathbf{x}_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\boldsymbol{\\epsilon}_t, t)\\|^2 \\Big]\r\\end{aligned}\r$$ Ho et al. (2020) found that training diffusion models performs better by discarding weights, specifically:\n$$\r\\begin{aligned}\rL_\\text{simple} \u0026= \\mathbb{E}_{t \\sim [1, T], \\mathbf{x}_0, \\boldsymbol{\\epsilon}_t} \\Big[\\|\\boldsymbol{\\epsilon}_t - \\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t)\\|^2 \\Big] \\\\\r\u0026= \\mathbb{E}_{t \\sim [1, T], \\mathbf{x}_0, \\boldsymbol{\\epsilon}_t} \\Big[\\|\\boldsymbol{\\epsilon}_t - \\boldsymbol{\\epsilon}_\\theta(\\sqrt{\\bar{\\alpha}_t}\\mathbf{x}_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\boldsymbol{\\epsilon}_t, t)\\|^2 \\Big]\r\\end{aligned}\r$$ Conclusion Diffusion models have revolutionized generative modeling, offering robust performance across diverse tasks. Their adaptability and scalability make them a valuable tool in modern machine learning. As research continues to advance, we can expect even greater improvements in the efficiency and capabilities of these models.\nReferences Weng, Lilian. \u0026ldquo;What are diffusion models?\u0026rdquo; Lil’Log (2021) https://lilianweng.github.io/posts/2021-07-11-diffusion-models/. Ho, Jonathan, Ajay Jain, and Pieter Abbeel. \u0026ldquo;Denoising Diffusion Probabilistic Models.\u0026rdquo; arXiv preprint arXiv:2006.11239 (2020). Song, Jiaming, and Stefano Ermon. \u0026ldquo;Generative Modeling by Estimating Gradients of the Data Distribution.\u0026rdquo; arXiv preprint arXiv:1907.05600 (2019). Dhariwal, Prafulla, and Alexander Nichol. \u0026ldquo;Diffusion Models Beat GANs on Image Synthesis.\u0026rdquo; arXiv preprint arXiv:2105.05233 (2021). Sohl-Dickstein, Jascha, et al. \u0026ldquo;Deep Unsupervised Learning using Nonequilibrium Thermodynamics.\u0026rdquo; arXiv preprint arXiv:1503.03585 (2015). Kingma, Diederik P., and Max Welling. \u0026ldquo;Auto-Encoding Variational Bayes.\u0026rdquo; arXiv preprint arXiv:1312.6114 (2013). ","permalink":"https://quyenmvo.github.io/blogs/diffusion-model/","summary":"Introduction Diffusion models have emerged as one of the most advanced methods in the field of generative modeling. They have broken the long-standing dominance of GANs in image generation tasks and have shown potential in various other fields, including computer vision, natural language processing, temporal data modeling, multimodal modeling, machine learning, and interdisciplinary applications such as computational chemistry and medical image reconstruction.\nDiffusion models have increasingly achieved significant successes across multiple domains.","title":"Diffusion Models: An In-depth Overview"},{"content":"Overview This project explores the enhancement of text-to-image generation models by integrating multimodal large language models (MLLMs) for prompt enhancement and feedback simulation. The project is divided into two main stages:\nImproving text-to-image generation with MLLMs Fine-tuning text-to-image models with active learning Introduction The goal of this project is to improve the quality and accuracy of images generated from textual descriptions. By leveraging MLLMs, we can refine user input prompts and provide simulated feedback to continuously enhance the model\u0026rsquo;s performance.\nMethodology Stage 1: Improving Text-to-Image Generation In this stage, we integrate MLLMs to enhance user prompts and verify the generated images for better alignment with user requests. The pipeline involves:\nPrompt Enhancement: Utilizing MLLMs to refine user input text. Image Generation: Generating images based on the enhanced prompts. Image Verification: Using MLLMs to validate the generated images and ensure they meet user expectations. Stage 2: Fine-Tuning with Active Learning This stage incorporates an active learning framework where the text-to-image model generates images and MLLMs simulate user feedback. The process includes:\nFeedback Simulation: MLLMs provide scores for generated images based on user preferences. Model Fine-Tuning: Using reinforcement learning to adjust the model parameters based on feedback scores. Iterative Refinement: Continuously refining the model through multiple iterations of generation and scoring. Experiments and Results Experiments were conducted on the MS-COCO dataset to evaluate the performance of the proposed methods. The results demonstrated significant improvements in image quality and alignment with user prompts compared to traditional models.\nStage 1 Results The first pipeline showed improved image generation quality by enhancing the prompts and ensuring better alignment with textual descriptions.\nStage 2 Results The second pipeline further fine-tuned the model using simulated feedback from MLLMs, resulting in continuous improvement in image quality and alignment with user preferences over multiple iterations.\nConclusion This project highlights the potential of using MLLMs and active learning to significantly enhance the performance of text-to-image generation models. The integration of advanced AI techniques allows for more accurate and user-aligned image outputs.\nAcknowledgements I would like to express my deep gratitude to my supervisors, Mr. Nguyễn Quang Đức and Dr. Nguyễn Đức Dũng, for their guidance and support throughout the research process. I also thank the Faculty of Computer Science and Engineering at the Ho Chi Minh City University of Technology for providing the knowledge and resources necessary to complete this project.\nGitHub Repository You can find the full project and codebase on GitHub: Project Repository\n","permalink":"https://quyenmvo.github.io/projects/improving-t2i-models-with-ai-feedback/","summary":"Overview This project explores the enhancement of text-to-image generation models by integrating multimodal large language models (MLLMs) for prompt enhancement and feedback simulation. The project is divided into two main stages:\nImproving text-to-image generation with MLLMs Fine-tuning text-to-image models with active learning Introduction The goal of this project is to improve the quality and accuracy of images generated from textual descriptions. By leveraging MLLMs, we can refine user input prompts and provide simulated feedback to continuously enhance the model\u0026rsquo;s performance.","title":"Improving Text-to-Image Models with Artificial Intelligence Feedback"}]